from lexer_code import tokenize
from parser_code import to_postfix, postfix_to_ast
from nfa_builder import build_nfa
from nfa_simulator import simulate_nfa

def run_regex_engine(regex: str, input_str: str):
    print(f"\nğŸ” Input Regex: {regex}")
    print(f"ğŸ”¢ Input String: {input_str}")

    # Step 1: Tokenize
    tokens = tokenize(regex)
    print("ğŸ§© Tokens:", tokens)

    # Step 2: Postfix conversion
    postfix = to_postfix(tokens)
    print("ğŸ“¤ Postfix:", postfix)

    # Step 3: AST from Postfix
    ast = postfix_to_ast(postfix)
    print("ğŸŒ³ AST built.")

    # Step 4: Build Îµ-NFA from AST
    nfa = build_nfa(ast)
    print("ğŸ—ï¸  NFA constructed.")

    # Step 5: Simulate the NFA
    result = simulate_nfa(nfa, input_str)
    print("âœ… Match!" if result else "âŒ No match.")

if __name__ == "__main__":
    # Sample input
    regex = input("Enter regex: ")
    string = input("Enter input string: ")
    run_regex_engine(regex, string)
